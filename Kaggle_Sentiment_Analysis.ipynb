{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLogxR8XOCkD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow import keras as tf\n",
        "import tensorflow as T\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz3eq9bqRYDl",
        "outputId": "de7b3175-01c7-41c2-9d90-57a0f506e830"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCbbk-qMQz5L"
      },
      "source": [
        "# Augmenting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGiV5_0sQ4mi"
      },
      "outputs": [],
      "source": [
        "def augment_neutral_class(df, n):\n",
        "    neutral = df[df['target'] == 1]\n",
        "    new_instances = []\n",
        "    for i in range(2, 11):\n",
        "        for _ in range(n):\n",
        "            instance = neutral.sample(n=i, replace=True)\n",
        "            augmented_data = ', '.join(instance['text'].values)\n",
        "            if augmented_data.count(' ') < 374:\n",
        "                new_instances.append(augmented_data)\n",
        "    x = list(set(new_instances))\n",
        "    if len(x) > 500000:\n",
        "      return x[:500000]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZB8okBIM_Oo"
      },
      "source": [
        "# Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqGWFQSzMT57"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/IFT6390/Dataset/dataset_final_fr.csv')\n",
        "#dataset = pd.read_csv('/content/drive/MyDrive/Kaggle-2/dataset.csv')\n",
        "\n",
        "# augmented_neutral_data = augment_neutral_class(dataset, 100000)\n",
        "# dataset = dataset.append(pd.DataFrame({'text': augmented_neutral_data, 'target': 1}), ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97E7W3JMGnIA"
      },
      "outputs": [],
      "source": [
        "vocab_size = 25000\n",
        "maxlen = 374\n",
        "\n",
        "X = dataset['text'].astype(str).values.tolist()\n",
        "tokenizer = tf.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = tf.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "Y = tf.utils.to_categorical(dataset['target'].values.tolist(), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtjthqczHb1_"
      },
      "outputs": [],
      "source": [
        "initializer = tf.initializers.HeNormal()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEOV-wPSTAbw"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(tf.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = tf.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim, kernel_initializer=initializer)\n",
        "\n",
        "        self.ffn = tf.Sequential(\n",
        "            [\n",
        "                tf.layers.Dense(ff_dim, activation=\"relu\", kernel_initializer=initializer), \n",
        "                tf.layers.Dense(embed_dim, kernel_initializer=initializer),\n",
        "            ] # Attention is all you need : Just uses activation in between\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = tf.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(tf.layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = tf.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = tf.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = T.shape(x)[-1]\n",
        "        positions = T.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Shaep81TPnZ"
      },
      "outputs": [],
      "source": [
        "def transformer():\n",
        "  embed_dim = 32  \n",
        "  num_heads = 2\n",
        "  ff_dim = 512 \n",
        "\n",
        "  embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "  # transformer_block_1 = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "\n",
        "  inputs = tf.layers.Input(shape=(maxlen))\n",
        "  x_ip = embedding_layer(inputs)\n",
        "  # x = transformer_block_1(x_ip)\n",
        "  # tx = tf.layers.Dropout(0.1)(x)\n",
        "  # tx = tf.layers.GlobalMaxPool1D()(tx)\n",
        "  # tx = tf.layers.Dense(512, 'relu', kernel_initializer=initializer)(tx)\n",
        "  # tx = tf.layers.Dense(128, 'relu', kernel_initializer=initializer)(tx)\n",
        "\n",
        "  lx = tf.layers.LSTM(128, return_sequences=True, kernel_initializer=initializer)(x_ip)\n",
        "  # lx = tf.layers.Flatten()(lx)\n",
        "  lx = tf.layers.LSTM(128, kernel_initializer=initializer)(lx)\n",
        "\n",
        "  cx = tf.layers.Conv1D(filters=128, kernel_size=4, activation=\"relu\", kernel_initializer=initializer)(x_ip)\n",
        "  cx = tf.layers.Conv1D(filters=128, kernel_size=4, activation=\"relu\", kernel_initializer=initializer)(cx)\n",
        "  cx = tf.layers.GlobalMaxPooling1D()(cx)\n",
        "\n",
        "  x = tf.layers.Multiply()([lx, cx])\n",
        "  x = tf.layers.Dropout(0.1)(x)\n",
        "  x = tf.layers.Dense(1024, 'relu', kernel_initializer=initializer)(x)\n",
        "  x = tf.layers.Dense(2700, 'relu', kernel_initializer=initializer)(x)\n",
        "\n",
        "  x_2d = tf.layers.Reshape((30,30,3))(x)\n",
        "  cx_2d = tf.layers.Conv2D(16,3, activation='relu', kernel_initializer=initializer)(x_2d)\n",
        "  cx_2d = tf.layers.Conv2D(8,3, activation='relu', kernel_initializer=initializer)(cx_2d)\n",
        "  cx_2d = tf.layers.Conv2D(3,5, activation='relu', kernel_initializer=initializer)(cx_2d)\n",
        "  x = tf.layers.Conv1D(filters=256, kernel_size=4, activation=\"relu\", kernel_initializer=initializer)(cx_2d)\n",
        "  x = tf.layers.Flatten()(x)\n",
        "  x = tf.layers.Dense(128, kernel_initializer=initializer, activation=\"relu\")(x)\n",
        "\n",
        "  outputs = tf.layers.Dense(3, kernel_initializer=initializer, activation=\"softmax\")(x)\n",
        "\n",
        "  model = tf.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "model = transformer()\n",
        "model.compile('adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjnNDJuA_gSD",
        "outputId": "56b30265-51ed-4eeb-b0b1-fc941884b743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1040323, 374) (1040323, 3)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, Y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWL8HXHNTuzQ",
        "outputId": "b8bbef82-6266-4583-bcc4-1ea20ad44d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  77/1524 [>.............................] - ETA: 18:39:52 - loss: 0.7247 - accuracy: 0.5562"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    X,\n",
        "    Y, \n",
        "    batch_size=512, \n",
        "    epochs=1, \n",
        "    validation_split=0.25\n",
        ")\n",
        "# Having validation loss < training loss might be counter-intuitive, it may be because of dropout layers in the model which\n",
        "# causes the model to behave differently during training and inference. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bbyi3ftpi2A0"
      },
      "outputs": [],
      "source": [
        "model.save('/content/drive/MyDrive/Kaggle-2/Transformer-30-11-2.0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0Idz-NuT7mP",
        "outputId": "c39f5267-ac98-4764-a99d-029d31342dd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(560175, 1)\n",
            "560175\n"
          ]
        }
      ],
      "source": [
        "# dataset = pd.read_csv('/content/drive/MyDrive/IFT6390/Kaggle-2/test.csv')\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Kaggle-2/test.csv')\n",
        "print(dataset.shape)\n",
        "test = dataset['text'].astype(str).values.tolist()\n",
        "print(len(test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N239CvfGm9TO",
        "outputId": "bb0eaaf6-3167-4ba6-ed02-fcda038e5ff8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17506/17506 [==============================] - 372s 21ms/step\n"
          ]
        }
      ],
      "source": [
        "sequences = tokenizer.texts_to_sequences(test)\n",
        "test = tf.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
        "predictions = model.predict(np.array(test))\n",
        "preds = np.argmax(predictions, 1)\n",
        "csv = 'id,target\\n'\n",
        "for id, pred in enumerate(preds):\n",
        "  csv += '{},{}\\n'.format(id, pred)\n",
        "# with open('/content/drive/MyDrive/IFT6390/Kaggle-2/predictions_transformer_modify.csv', 'w') as f:\n",
        "with open('/content/drive/MyDrive/Kaggle-2/predictions_transformer_4.csv', 'w') as f:\n",
        "  f.writelines(csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgvbMRxMNFMu"
      },
      "source": [
        "# LSTM+CNN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smcb9xAVh3GJ"
      },
      "outputs": [],
      "source": [
        "# dataset = pd.read_csv('/content/drive/MyDrive/IFT6390/Kaggle-2/dataset.csv')\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/Kaggle-2/dataset.csv')\n",
        "\n",
        "augmented_neutral_data = augment_neutral_class(dataset, 10000)\n",
        "dataset = dataset.append(pd.DataFrame({'text': augmented_neutral_data, 'target': 1}), ignore_index=True)\n",
        "\n",
        "vocab_size = 20000\n",
        "maxlen = 374\n",
        "\n",
        "# X = dataset['text'].values.tolist()\n",
        "# tokenizer = tf.preprocessing.text.Tokenizer(num_words=vocab_size)\n",
        "# tokenizer.fit_on_texts(X)\n",
        "# sequences = tokenizer.texts_to_sequences(X)\n",
        "# X = tf.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Y = tf.utils.to_categorical(dataset['target'].values.tolist(), 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiq9IAKBIWDY"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGuC2OKYI8qR",
        "outputId": "30eaa1a1-3f09-4583-87fc-f8bf7a2515f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb7Vji80I-17"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras import models\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCOEOVzEJEOb"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = stopwords.words('english')\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
        "\n",
        "def remove_mentions(text):\n",
        "    return ' '.join([word for word in text.split() if not word.startswith('@')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpaMeOsqJGhD",
        "outputId": "2e27379c-c506-4f9d-fbee-1178a3c4a505"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1125596\n",
            "(1125596, 2)\n"
          ]
        }
      ],
      "source": [
        "# View size of train new data\n",
        "print(len(dataset))\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T__yyVtLJPvL"
      },
      "outputs": [],
      "source": [
        "train_new=dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wjgHOVPJLvJ"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords and mentions from text column in train_new\n",
        "train_new['text'] = train_new['text'].apply(remove_stopwords).apply(remove_mentions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LhlVguIJN-e"
      },
      "outputs": [],
      "source": [
        "# Use WordtoVec to convert text to vectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_new.text, train_new.target, test_size=0.1, random_state=37)\n",
        "tk = Tokenizer(num_words=20000, lower=True, split=' ')\n",
        "tk.fit_on_texts(X_train)\n",
        "X_train_seq = tk.texts_to_sequences(X_train)\n",
        "X_test_seq = tk.texts_to_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dmgo33EBJXRs"
      },
      "outputs": [],
      "source": [
        "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=374)\n",
        "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=374)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGzM4bPrJeF8"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "y_train_le = le.fit_transform(y_train)\n",
        "y_test_le = le.transform(y_test)\n",
        "y_train_oh = to_categorical(y_train_le)\n",
        "y_test_oh = to_categorical(y_test_le)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MWscLXgJfyO"
      },
      "outputs": [],
      "source": [
        "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73tuVF86Jjwi",
        "outputId": "09f9eab0-f08b-4c99-f5f6-7edfe3869665"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1099/1781 [=================>............] - ETA: 4:39 - loss: 0.6102 - accuracy: 0.6950"
          ]
        }
      ],
      "source": [
        "emb_model = models.Sequential()\n",
        "emb_model.add(layers.Embedding(10000, 8, input_length=374))\n",
        "# emb_model.add(layers.Flatten())\n",
        "emb_model.add(layers.LSTM(512, dropout=0.2))\n",
        "emb_model.add(layers.Dense(3, activation='softmax'))\n",
        "emb_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "emb_model.fit(X_train_emb, y_train_emb, epochs=10, batch_size=512, validation_data=(X_valid_emb, y_valid_emb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S55eo78qMBvp",
        "outputId": "2d0cd002-c9ee-4293-970d-45817960a184"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3166/3166 [==============================] - 6s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_pred = emb_model.predict(X_valid_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNuajrl-Pb1M"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOWNEpYeo4Px"
      },
      "source": [
        "## CNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbKcZv-DhbK4"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/IFT6390/Dataset/test_final_fr.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eLB11P2pwMX"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv1EoV3gg4TJ"
      },
      "outputs": [],
      "source": [
        "max_len = 0\n",
        "for i in dataset['text']:\n",
        "  split_i = i.split()\n",
        "  if len(split_i) > max_len:\n",
        "    max_len = len(split_i)\n",
        "\n",
        "for j in test['text']:\n",
        "  split_j = j.split()\n",
        "  if len(split_j) > max_len:\n",
        "    max_len = len(split_j)\n",
        "    \n",
        "print('Max length of texts :', max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QAvOabFfiUvp"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT_lwGubg4Po"
      },
      "outputs": [],
      "source": [
        "max_fatures = 300000 # the number of words to be used for the input of embedding layer\n",
        "tokenizer = Tokenizer(num_words=max_fatures, split=' ') #Create the instance of Tokenizer\n",
        "tokenizer.fit_on_texts(dataset['text'].values)\n",
        "train_converted = tokenizer.texts_to_sequences(dataset['text'].values)\n",
        "# test = tokenizer.texts_to_sequences(test['text'].values)\n",
        "train_converted = pad_sequences(train_converted, maxlen=max_len) # Turning the vectors of train data into sequences \n",
        "#test = pad_sequences(test, maxlen=max_len) # Turning the vectors of test data into sequences "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWSngjon7-Gh"
      },
      "outputs": [],
      "source": [
        "test = tokenizer.texts_to_sequences(test['text'].values)\n",
        "test = pad_sequences(test, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1y4AYmolwiZ"
      },
      "outputs": [],
      "source": [
        "target_converted = pd.get_dummies(dataset['target']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3tQINyQGmP40"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm493-PZpwJL"
      },
      "outputs": [],
      "source": [
        "# Make sure that the shape of train and test data are same\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(train_converted, target_converted, test_size = 0.1, random_state = 42)\n",
        "\n",
        "# Use half of the test data for validation during training\n",
        "validation_size = 50000\n",
        "# validation_size = 500\n",
        "X_validate = X_test[-validation_size:]\n",
        "Y_validate = Y_test[-validation_size:]\n",
        "X_test = X_test[:-validation_size]\n",
        "Y_test = Y_test[:-validation_size]\n",
        "\n",
        "print('The shape of train data :', X_train.shape)\n",
        "print('The shape of labels of train data :', Y_train.shape)\n",
        "print('The shape of test data :', X_test.shape)\n",
        "print('The shape of test label data :', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJp5QNtlnJVK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SpatialDropout1D, Embedding, LSTM, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njBlGCZp7rLJ"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "NUM_FILTERS = 256 # Number of filters\n",
        "NUM_WORDS = 4 # Number of the words to be convoluted\n",
        "embed_dim = 512 # The size of the vector space where words will be embedded\n",
        "batch_size = 512\n",
        "EPOCHS = 2\n",
        "\n",
        "# Create the CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_fatures, embed_dim, input_length = train_converted.shape[1]))\n",
        "model.add(SpatialDropout1D(0.5))\n",
        "model.add(Conv1D(filters=NUM_FILTERS, kernel_size=NUM_WORDS, activation=\"relu\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(3, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",metrics=[\"acc\"])\n",
        "print(model.summary()) # Show the summary of the model\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=batch_size,\n",
        "                    epochs=EPOCHS, validation_data=(X_validate, Y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTPAfrgPpwCl"
      },
      "outputs": [],
      "source": [
        "train_acc = history.history['acc']\n",
        "test_acc = history.history['val_acc']\n",
        "x = np.arange(len(train_acc))\n",
        "plt.plot(x, train_acc, label = 'train accuracy')\n",
        "plt.plot(x, test_acc, label = 'test accuracy')\n",
        "plt.title('Train and validation accuracy')\n",
        "plt.xlabel('Number of epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xDsHr-sBVgM"
      },
      "outputs": [],
      "source": [
        "score, acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
        "print(\"The loss of this model: %.2f\" % (score))\n",
        "print(\"The accuracy of this model: %.2f\" % (acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBrTXpM3BcRP"
      },
      "outputs": [],
      "source": [
        "# Let's predict whether the tweets are positive or negative using real test data set \n",
        "predictions = model.predict(test)\n",
        "# Let's show the first 5 predictions as an samples\n",
        "print('Prediction samples', predictions[:5])\n",
        "# Show the shape of prediction, and the number of rows should be same to number of test data\n",
        "print('The shape of predictions:', predictions.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Let's turn the prediction from [0, 1] and [1, 0] into 0(negative) and 4(positive)\n",
        "# prediction_final = []\n",
        "# for each_pediction in prediction_binary:\n",
        "#   if each_pediction[0] == 1:\n",
        "#     prediction_final.append(0)\n",
        "#   else:\n",
        "#     prediction_final.append(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpu5a1LYD03e"
      },
      "outputs": [],
      "source": [
        "predictions.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYPeYmtHHLao"
      },
      "outputs": [],
      "source": [
        "sentiment = []\n",
        "for i in range(len(predictions)):\n",
        "    sentiment.append(np.argmax(predictions[i]))\n",
        "# Show the first 5 sentiment values\n",
        "print('Sentiment samples:', sentiment[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGYELHraIEUn"
      },
      "outputs": [],
      "source": [
        "submission = pd.DataFrame({'id': range(len(sentiment)), 'target': sentiment})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZkrHDL2DN-Q"
      },
      "outputs": [],
      "source": [
        "submission.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n944mnO4Im4h"
      },
      "outputs": [],
      "source": [
        "submission.to_csv('submission_CNN_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvAUXWRpTJ14"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgEEdvqRMbOF",
        "outputId": "724af19e-4e1e-4488-ff63-940f200ef1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1540323, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[:200000]"
      ],
      "metadata": {
        "id": "m8rY_9DGNSFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcE-MP3aRN5M",
        "outputId": "94172c52-a9ba-4564-8527-bb9386bcb8aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "j9v9BMYz110Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentences_to_words(text):\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \",text) \n",
        "    words = letters_only.lower().split()                             \n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    meaningful_words = [w for w in words if not w in stops] \n",
        "    return( \" \".join( meaningful_words ))"
      ],
      "metadata": {
        "id": "835_8yZB1RKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "data['clean_text']=data['text'].apply(lambda x: sentences_to_words(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCRVa0cl1_8s",
        "outputId": "2422dd05-4999-478a-bf86-f74cd4f7e142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-10-6c22bbbbf770>:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data['clean_text']=data['text'].apply(lambda x: sentences_to_words(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = data['clean_text']\n",
        "y = data['target']\n",
        "\n",
        "print(len(x),len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9e0DRYV1RIM",
        "outputId": "7e93374d-b183-4912-cecf-d65cdd2d5b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000 200000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "p-zF8giv3cq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42)\n",
        "print(len(x_train), len(y_train))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGaRqFYj1RFp",
        "outputId": "a667fca3-1bbf-4a0d-c9f8-5d42d8362fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150000 150000\n",
            "50000 50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# instantiate the vectorizer\n",
        "vect = CountVectorizer()\n",
        "vect.fit(x_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9YWPs_r1RCz",
        "outputId": "b79ee9b7-fc8c-4d73-db72-4af79c3ab9e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained to create a document-term matrix from train and test sets\n",
        "x_train_dtm = vect.transform(x_train)\n",
        "x_test_dtm = vect.transform(x_test)\n"
      ],
      "metadata": {
        "id": "8m-oCp1g1RAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect_tunned = CountVectorizer(stop_words='english', ngram_range=(1,2), min_df=0.1, max_df=0.7, max_features=100)\n",
        "vect_tunned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NafHRd0N2uS1",
        "outputId": "f108dc3d-55f0-4341-da50-c3eb88fbb874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.7, max_features=100, min_df=0.1, ngram_range=(1, 2),\n",
              "                stop_words='english')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "model = SVC(kernel='linear', random_state = 10)\n",
        "model.fit(x_train_dtm, y_train)\n",
        "#predicting output for test data\n",
        "pred = model.predict(x_test_dtm)"
      ],
      "metadata": {
        "id": "V57IznvJ2z9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "id": "vHP4TneD22Lz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8ea4e2a-67a2-47bd-b528-fd154f68283b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 0, 0, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 2, 0,\n",
              "       0, 2, 0, 0, 2, 0, 0, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 2, 2, 2, 0, 0,\n",
              "       0, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 0, 2, 0,\n",
              "       2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2,\n",
              "       0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2,\n",
              "       0, 2, 2, 2, 2, 0, 0, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0,\n",
              "       2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0, 2, 0, 0,\n",
              "       2, 2, 0, 2, 0, 0, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0,\n",
              "       0, 2, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 0, 2, 2, 2, 2,\n",
              "       2, 0, 0, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 2, 2, 0, 0, 0, 2, 2, 2,\n",
              "       0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2,\n",
              "       0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0,\n",
              "       2, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0,\n",
              "       0, 2, 2, 0, 2, 2, 0, 2, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 2, 2,\n",
              "       0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2,\n",
              "       0, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 0,\n",
              "       0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0,\n",
              "       2, 0, 0, 2, 2, 2, 2, 0, 2, 2, 2, 2, 0, 0, 0, 0, 2, 2, 0, 0, 2, 0,\n",
              "       2, 2, 2, 0, 0, 2, 0, 0, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 2,\n",
              "       2, 0, 2, 0, 0, 2, 0, 0, 2, 2, 2, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 0,\n",
              "       0, 0, 2, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset"
      ],
      "metadata": {
        "id": "1kjSSXWxPaGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf=TfidfVectorizer()\n",
        "text_tf= tf.fit_transform(data['text'])"
      ],
      "metadata": {
        "id": "okFJd95RPW8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_tf = tf.fit_transform(test['text'])"
      ],
      "metadata": {
        "id": "SFSc_ypnQuIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    text_tf, data['target'], test_size=0.3, random_state=123)"
      ],
      "metadata": {
        "id": "-_6NQY3FPW5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import metrics\n",
        "# Model Generation Using Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(X_train, y_train)\n",
        "predicted= clf.predict(X_test)\n",
        "print(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKTHyCAVPW3J",
        "outputId": "b1b1969f-65ce-447d-a7d1-04356b84aa59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB Accuracy: 0.8444092906900499\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBykHxbqQFBg",
        "outputId": "706c97fc-7696-4a0c-e3cc-503f8713c169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 2, ..., 2, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/content/drive/MyDrive/IFT6390/Kaggle-2/test.csv')\n",
        "#test = pd.read_csv('/content/drive/MyDrive/Kaggle-2/test.csv')\n",
        "print(dataset.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zr2KoXn9PW0w",
        "outputId": "cbf86053-7964-418b-cde1-4cfc9688e9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1540323, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = clf.predict(test_tf)"
      ],
      "metadata": {
        "id": "XfgN01IwQmn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "preds = np.argmax(predictions, 1)\n",
        "csv = 'id,target\\n'\n",
        "for id, pred in enumerate(preds):\n",
        "  csv += '{},{}\\n'.format(id, pred)\n",
        "# with open('/content/drive/MyDrive/IFT6390/Kaggle-2/predictions_transformer_modify.csv', 'w') as f:\n",
        "with open('/content/drive/MyDrive/Kaggle-2/predictions_transformer_4.csv', 'w') as f:\n",
        "  f.writelines(csv)"
      ],
      "metadata": {
        "id": "0PEIVly6PWxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "PicpMdzsGTDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "LJ7J7pt9GSiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/IFT6390/Dataset/dataset_final_fr.csv')"
      ],
      "metadata": {
        "id": "VBMeJ6BeMjnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset"
      ],
      "metadata": {
        "id": "on90G9RzGoJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ACawB7gtGrkq",
        "outputId": "64ac16f4-38e1-45c2-f964-f5253607db88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               text  target\n",
              "0   0                    anyway im geting of for a while       2\n",
              "1   1  my red apache isn't felin to wel this morning htp       0\n",
              "2   2  @user you should be its great friday wil be gr...       2\n",
              "3   3  its pm and i dont wana slep so i debated with ...       2\n",
              "4   4               why does twiter eat my dm's not hapy       0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3dfd63d9-aaaa-415f-80c8-40c685252621\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>anyway im geting of for a while</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>my red apache isn't felin to wel this morning htp</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>@user you should be its great friday wil be gr...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>its pm and i dont wana slep so i debated with ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>why does twiter eat my dm's not hapy</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3dfd63d9-aaaa-415f-80c8-40c685252621')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3dfd63d9-aaaa-415f-80c8-40c685252621 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3dfd63d9-aaaa-415f-80c8-40c685252621');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of string kernel\n",
        "# compare poisition-wise 2 sequences(X & Y) and return similarity score.\n",
        "def equal_elements(s1,s2):\n",
        "    score = 0\n",
        "    for i in range(len(s1)):\n",
        "        score += (s1[i] == s2[i])*1 # This is an unoptimized way to do this. \n",
        "    return score\n",
        "\n",
        "equal_elements(\"STRING\",\"KERNEL\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HY2FEMSHYMG",
        "outputId": "91df8f7d-e61e-4a36-b94a-1dca065d9aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SVC(kernel=equal_elements)\n",
        "clf.fit(data['text'],data['target']) # this producecs an error"
      ],
      "metadata": {
        "id": "D9R_xalYHwZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[['text','target']]"
      ],
      "metadata": {
        "id": "BJwA_i8gIE1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.iloc[1,0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ejVa8bRKH-yi",
        "outputId": "c6d3d932-8a65-4dc5-a588-28ae7ea7b2a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"my red apache isn't felin to wel this morning htp\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size = 12\n",
        "not_so_good_string_kernel = np.zeros((size, size))\n",
        "for row in range(size):\n",
        "    for column in range(size):\n",
        "        not_so_good_string_kernel[row,column] = equal_elements(data.iloc[row, 0],data.iloc[column, 0])\n",
        "not_so_good_string_kernel"
      ],
      "metadata": {
        "id": "SnLIkydeHLTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compose_kernel(row_idxs, col_idxs):\n",
        "    row_idxs = np.array(row_idxs).astype(np.int)\n",
        "    col_idxs = np.array(col_idxs).astype(np.int)\n",
        "    select_kernel = np.zeros((len(row_idxs),len(col_idxs)))\n",
        "    for i, row_idx in enumerate(row_idxs):\n",
        "        for j, col_idx in enumerate(col_idxs):\n",
        "            select_kernel[i,j] = not_so_good_string_kernel[row_idx,col_idx]  # Change to custom distance kernel\n",
        "    \n",
        "    return select_kernel\n",
        "\n",
        "compose_kernel([5,2,3,1],[5,2,3,1]) # random example"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGhrLfEXHV3R",
        "outputId": "70b25a6e-edbd-458f-cf4d-80fb47833285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-497a1e4840c1>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  row_idxs = np.array(row_idxs).astype(np.int)\n",
            "<ipython-input-24-497a1e4840c1>:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  col_idxs = np.array(col_idxs).astype(np.int)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = data['target'].values\n",
        "X_train_idx, X_test_idx, y_train, y_test = train_test_split(np.arange(size),y[:size], test_size=4) # OR USE KFoldStratified()\n",
        "X_train_idx, X_test_idx, y_train, y_test"
      ],
      "metadata": {
        "id": "uauvJ9N3KhYO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "HOWNEpYeo4Px"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}